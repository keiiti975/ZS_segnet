{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\*\\*this code can test MSE_decoder_test setting\\*\\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports without torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path\n",
    "import visdom\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import sys\n",
    "\n",
    "##########\n",
    "# imports torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "##########\n",
    "\n",
    "# imports models\n",
    "import model.segnet as segnet\n",
    "import model.encoder as encoder  # label->word\n",
    "import model.decoder as decoder  # word->label\n",
    "import zs_dataset_list as datasets\n",
    "\n",
    "# imports utility\n",
    "import make_log as flog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###mapping function and GT_list###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_map_te=np.asarray([\n",
    "26,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "4,\n",
    "3,\n",
    "3,\n",
    "3,\n",
    "3,\n",
    "3,\n",
    "3,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "2,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "1,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "9,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "8,\n",
    "7,\n",
    "7,\n",
    "7,\n",
    "7,\n",
    "7,\n",
    "7,\n",
    "6,\n",
    "6,\n",
    "6,\n",
    "6,\n",
    "6,\n",
    "6,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "5,\n",
    "19,\n",
    "19,\n",
    "15,\n",
    "17,\n",
    "17,\n",
    "15,\n",
    "20,\n",
    "16,\n",
    "25,\n",
    "22,\n",
    "23,\n",
    "23,\n",
    "19,\n",
    "19,\n",
    "14,\n",
    "20,\n",
    "20,\n",
    "19,\n",
    "20,\n",
    "12,\n",
    "20,\n",
    "16,\n",
    "22,\n",
    "22,\n",
    "22,\n",
    "22,\n",
    "22,\n",
    "15,\n",
    "11,\n",
    "18,\n",
    "18,\n",
    "20,\n",
    "15,\n",
    "12,\n",
    "12,\n",
    "13,\n",
    "17,\n",
    "15,\n",
    "20,\n",
    "19,\n",
    "25,\n",
    "20,\n",
    "15,\n",
    "13,\n",
    "12,\n",
    "19,\n",
    "16,\n",
    "25,\n",
    "12,\n",
    "19,\n",
    "15,\n",
    "25,\n",
    "12,\n",
    "12,\n",
    "16,\n",
    "12,\n",
    "11,\n",
    "12,\n",
    "13,\n",
    "17,\n",
    "19,\n",
    "18,\n",
    "12,\n",
    "11,\n",
    "20,\n",
    "14,\n",
    "17,\n",
    "12,\n",
    "13,\n",
    "20,\n",
    "13,\n",
    "15,\n",
    "16,\n",
    "20,\n",
    "17,\n",
    "19,\n",
    "19,\n",
    "15,\n",
    "18,\n",
    "24,\n",
    "24,\n",
    "24,\n",
    "24,\n",
    "24,\n",
    "24,\n",
    "24,\n",
    "11,\n",
    "11,\n",
    "21,\n",
    "21,\n",
    "13,\n",
    "26,\n",
    "])\n",
    "GT_list = [35, 26, 23, 9, 1, 83, 77, 72, 61, 51, 43, 154, 148,\n",
    "           149, 105, 123, 112, 127, 152, 167, 109, 179, 116, 102,\n",
    "           175, 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p_args:\n",
    "    load=True\n",
    "    config=\"MSE_decoder_test\"\n",
    "    test=True\n",
    "    no_cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': True, 'encoder': False, 'decoder': True, 'input_nbr': 3, 'target_nbr': 27, 'semantic_nbr': 100, 'batch_size': 12, 'epochs': 10, 'lr': 0.0001, 'momentum': 0.1, 'seed': 1, 'lamda': 0, 'gamma': 1, 'input_root': './data/train/input', 'target_root': './data/train/zs_target', 'map_root': './data/train/target', 'filenames': './data/train/zs_names.txt', 'semantic_filename': './v_class/class1.txt', 'project_dir': './model/MSE_decoder_test', 'model_load_pth': './model/MSE_batch12_wr/segnet.pth', 'head_load_pth': './model/decoder_batch2/segnet.pth', 'save_pth': 'segnet.pth', 'output_dir': './data/MSE_decoder_test', 'n_components': 100, 'PCA': False, 'cos_similarity': False, 'jaccard_similarity': False, 'ZSL': True, 'SSE': False}\n"
     ]
    }
   ],
   "source": [
    "# set config\n",
    "f_config = open(os.path.join(\"./config\", p_args.config + \".json\"), \"r\")\n",
    "args = json.load(f_config)\n",
    "for key, value in args.items():\n",
    "    if value == \"true\":\n",
    "        args[key] = True\n",
    "    elif value == \"false\":\n",
    "        args[key] = False\n",
    "print(args)\n",
    "\n",
    "# device settings\n",
    "p_args.cuda = not p_args.no_cuda and torch.cuda.is_available()\n",
    "USE_CUDA = p_args.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nbr = args[\"input_nbr\"]  # 入力次元数\n",
    "semantic_nbr = args[\"semantic_nbr\"]  # 特徴次元数\n",
    "target_nbr = args[\"target_nbr\"]  # 出力次元数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "if p_args.cuda:\n",
    "    torch.cuda.manual_seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = segnet.SegNet(input_nbr, semantic_nbr, args[\"momentum\"])\n",
    "head = decoder.ConvNet(semantic_nbr, target_nbr, args[\"momentum\"]) \n",
    "# semantic = 100, target = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegNet(\n",
      "  (conv11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn33): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv51): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn52): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv53): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn53): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv53d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn53d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv52d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn52d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv51d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn51d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv43d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn43d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv42d): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn42d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv41d): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn41d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv33d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn33d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv32d): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn32d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv31d): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn31d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv22d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn22d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv21d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn21d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv12d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn12d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv11d): Conv2d(64, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "ConvNet(\n",
      "  (conv1): Conv2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(100, 27, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if args[\"model\"] is True:\n",
    "    if USE_CUDA:  # convert to cuda if needed\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.float()\n",
    "    model.eval()\n",
    "    print(model)\n",
    "if args[\"encoder\"] is True or args[\"decoder\"] is True:\n",
    "    if USE_CUDA:  # convert to cuda if needed\n",
    "        head.cuda()\n",
    "    else:\n",
    "        head.float()\n",
    "    head.eval()\n",
    "    print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "# init window\n",
    "if p_args.test is False:\n",
    "    \"\"\"encoder or decoder\"\"\"\n",
    "    win = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_acc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_accuracy',\n",
    "            xlabel='epoch',\n",
    "            ylabel='accuracy',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log model\n",
    "f_log = flog.make_log(args[\"project_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ define(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(head.parameters(), lr=args[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_train(epoch, trainloader):\n",
    "    # set head to train mode\n",
    "    if args[\"model\"] is True:\n",
    "        model.eval()\n",
    "        head.train()\n",
    "    else:\n",
    "        head.train()\n",
    "\n",
    "    # define total_loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # define epoch_size\n",
    "    epoch_size = len(trainloader)\n",
    "\n",
    "    # define batch_loss\n",
    "    batch_loss = 0\n",
    "\n",
    "    # define lamda,gamma\n",
    "    lamda = args[\"lamda\"]\n",
    "    gamma = args[\"gamma\"]\n",
    "        \n",
    "    # define a loss\n",
    "    if USE_CUDA:\n",
    "        if args[\"decoder\"] is True:\n",
    "            \"\"\"decoder\"\"\"\n",
    "            loss = nn.CrossEntropyLoss(size_average=True).cuda()\n",
    "        else:\n",
    "            sys.exit(\"model is not defined\")\n",
    "        l1_loss = nn.L1Loss(size_average=False).cuda()\n",
    "    else:\n",
    "        if args[\"decoder\"] is True:\n",
    "            \"\"\"decoder\"\"\"\n",
    "            loss = nn.CrossEntropyLoss(size_average=True)\n",
    "        else:\n",
    "            sys.exit(\"model is not defined\")\n",
    "        l1_loss = nn.L1Loss(size_average=False)\n",
    "    \n",
    "    # define tqdm message\n",
    "    t = tqdm(trainloader, desc=\"loss\", leave=False)\n",
    "\n",
    "    # iteration over the batches\n",
    "    for batch_id, data in enumerate(t):\n",
    "        # make batch tensor and target tensor\n",
    "        if args[\"model\"] is True and args[\"decoder\"] is True:\n",
    "            input = data[\"input\"]\n",
    "            target = data[\"target\"]\n",
    "        elif args[\"decoder\"] is True:\n",
    "            input = data[\"input\"]\n",
    "            target = data[\"target\"]\n",
    "        else:\n",
    "            sys.exit(\"model is not defined\")\n",
    "\n",
    "        if USE_CUDA:\n",
    "            if args[\"model\"] is True and args[\"decoder\"] is True:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            elif args[\"decoder\"] is True:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            else:\n",
    "                sys.exit(\"model is not defined\")\n",
    "\n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predictions\n",
    "        if args[\"model\"] is True and args[\"decoder\"] is True:\n",
    "            semantic = model(input)\n",
    "            output = head(semantic)\n",
    "        elif args[\"decoder\"] is True:\n",
    "            output = head(input)\n",
    "        else:\n",
    "            sys.exit(\"model is not defined\")\n",
    "\n",
    "        # calculate loss\n",
    "        l_ = loss(output, target)\n",
    "        if lamda != 0:\n",
    "            reg_loss = 0\n",
    "            for param in head.parameters():\n",
    "                if USE_CUDA:\n",
    "                    param_target = torch.zeros(param.size()).cuda()\n",
    "                else:\n",
    "                    param_target = torch.zeros(param.size())\n",
    "                reg_loss += l1_loss(param, param_target)\n",
    "\n",
    "            reg_loss = lamda * reg_loss\n",
    "            l_ = l_ + reg_loss\n",
    "\n",
    "        total_loss += l_.item()\n",
    "        # backward loss\n",
    "        l_.backward()\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # train conditions\n",
    "        if lamda != 0:\n",
    "            t.set_description(\"reg_loss=%f, loss=%f\" % (reg_loss.item(), l_.item()))\n",
    "        else:\n",
    "            t.set_description(\"loss=%f\" % (l_.item()))\n",
    "\n",
    "        # visualize train conditions\n",
    "        if batch_id % 30 == 0 and batch_id != 0:\n",
    "            batch_loss = batch_loss + l_.item()\n",
    "            batch_loss = batch_loss / 30\n",
    "            # display visdom board\n",
    "            phase = epoch + batch_id / epoch_size\n",
    "            visualize(phase, batch_loss, win)\n",
    "            batch_loss = 0\n",
    "            # evaluate\n",
    "            \"\"\"decoder only\"\"\"\n",
    "            head.eval()\n",
    "            if args[\"model\"] is True and args[\"decoder\"] is True:\n",
    "                target = data[\"map\"]\n",
    "                output = head(semantic)\n",
    "            elif args[\"decoder\"] is True:\n",
    "                output = head(input)\n",
    "            else:\n",
    "                sys.exit(\"model is not defined\")\n",
    "            head_evaluate(output, target, epoch, epoch_size, batch_id)\n",
    "            head.train()\n",
    "        else:\n",
    "            batch_loss = batch_loss + l_.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_test(testloader):\n",
    "    # set head to eval mode\n",
    "    model.eval()\n",
    "    head.eval()\n",
    "\n",
    "    # make output_dir\n",
    "    if not os.path.isdir(args[\"output_dir\"]):\n",
    "        os.makedirs(args[\"output_dir\"])\n",
    "\n",
    "    # iteration over the batches\n",
    "    for batch_id, data in enumerate(tqdm(testloader)):\n",
    "        # make batch tensor and target tensor\n",
    "        input = data['input']\n",
    "\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda()\n",
    "\n",
    "        # predictions\n",
    "        semantic = model(input)\n",
    "        output = head(semantic)\n",
    "\n",
    "        # make result\n",
    "        filename = os.path.basename(\n",
    "            testloader.dataset.get_filename(batch_id)[0])\n",
    "        filename = filename.split(\".\")[0]\n",
    "        filename = filename + \".png\"\n",
    "        single_output = output[0, :, :, :]\n",
    "        te_result = single_output.max(0)[1].cpu().numpy()\n",
    "        result = np.uint8(te_result)\n",
    "\n",
    "        Image.fromarray(result).save(\n",
    "            os.path.join(args[\"output_dir\"], filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_evaluate(output, target_map, epoch, epoch_size, batch_id):\n",
    "    accuracy_te=[]\n",
    "    for id in range(output.size(0)):\n",
    "        single_output = output[id, :, :, :]\n",
    "        target = target_map[id, :, :].cpu().numpy()\n",
    "        msk = np.isin(target, GT_list)\n",
    "        ctarget = target.copy()\n",
    "        target[ctarget > 181] = 182\n",
    "        te_result = single_output.max(0)[1].cpu().numpy()\n",
    "        target = tr_map_te[target]\n",
    "        result = te_result[msk] == target[msk]\n",
    "        if len(result) != 0:\n",
    "            acc_te = result.mean()\n",
    "            accuracy_te.append(acc_te)\n",
    "\n",
    "    acc_te = np.asarray(accuracy_te)\n",
    "    acc = np.mean(acc_te[~np.isnan(acc_te)])\n",
    "    phase = epoch + batch_id / epoch_size\n",
    "    visualize(phase, acc, win_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(phase, visualized_data, window):\n",
    "    vis.line(\n",
    "        X=np.array([phase]),\n",
    "        Y=np.array([visualized_data]),\n",
    "        update='append',\n",
    "        win=window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_euclidean(out, sem):\n",
    "    \"\"\"pytorch calculate euclidean\"\"\"\n",
    "    nbr = sem.size(1)\n",
    "    ab = torch.mm(out.view(-1, nbr), sem.t())\n",
    "    ab = ab.view(out.size(0), out.size(1), sem.size(0))\n",
    "    aa = (sem**2).sum(1)\n",
    "    bb = (out**2).sum(-1)\n",
    "    res = aa[None, None, :] + bb[:, :, None] - 2 * ab\n",
    "    return res.min(-1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize from vgg16\n",
      "load weights from segnet.pth\n",
      "load weights from ConvNet.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f725d902df4d43b9a0b98d04d3632217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4990), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compose transforms\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomResizedCrop(256, scale=(1.0, 1.0),ratio=(1.0, 1.0),\n",
    "                                  interpolation=Image.NEAREST)]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    []\n",
    ")\n",
    "\n",
    "# initialize model\n",
    "model.initialized_with_pretrained_weights()\n",
    "if args[\"model\"] is True:\n",
    "    model.load_from_filename(args[\"model_load_pth\"])\n",
    "\n",
    "# load model\n",
    "if p_args.load is True:\n",
    "    if args[\"encoder\"] is True or args[\"decoder\"] is True:\n",
    "        head.load_from_filename(args[\"head_load_pth\"])\n",
    "\n",
    "# load dataset\n",
    "trainset = datasets.ImageFolderDenseFileLists(\n",
    "    input_root=args[\"input_root\"], target_root=args[\"target_root\"],\n",
    "    map_root=args[\"map_root\"], filenames=args[\"filenames\"],\n",
    "    semantic_filename=args[\"semantic_filename\"], training=True,\n",
    "    model=None, config=args, transform=train_transform,\n",
    "    USE_CUDA=USE_CUDA)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args[\"batch_size\"], shuffle=True,\n",
    "    num_workers=args[\"batch_size\"])\n",
    "testset = datasets.ImageFolderDenseFileLists(\n",
    "    input_root='./data/test/input', target_root=None,\n",
    "    map_root=None, filenames='./data/test/names.txt',\n",
    "    semantic_filename=args[\"semantic_filename\"], training=False,\n",
    "    model=None, config=args, transform=test_transform,\n",
    "    USE_CUDA=USE_CUDA)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# train and test\n",
    "for epoch in range(0, args[\"epochs\"]):\n",
    "\n",
    "    if p_args.test is False:\n",
    "        # training\n",
    "        loss = head_train(epoch, trainloader)\n",
    "        print(\"epoch:%d,head_loss:%f\" % (epoch,loss))\n",
    "        # open log_file\n",
    "        f_log.open()\n",
    "        # write log_file\n",
    "        f_log.write(epoch, loss)\n",
    "        # close log_file\n",
    "        f_log.close()\n",
    "        # make project_dir\n",
    "        if not os.path.isdir(args[\"project_dir\"]):\n",
    "            os.makedirs(args[\"project_dir\"])\n",
    "        # save checkpoint\n",
    "        torch.save(head.state_dict(),\n",
    "                   args[\"project_dir\"] + \"/checkpoint_\" + str(epoch) +\n",
    "                   \".pth\")\n",
    "    elif p_args.test is True and p_args.load is True:\n",
    "        # test\n",
    "        head_test(testloader)\n",
    "        break\n",
    "    else:\n",
    "        print('can not test the model!')\n",
    "        break\n",
    "# save model\n",
    "if p_args.test is False:\n",
    "    torch.save(head.state_dict(), os.path.join(\n",
    "        args[\"project_dir\"], args[\"save_pth\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
